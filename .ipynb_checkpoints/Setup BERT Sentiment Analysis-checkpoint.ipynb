{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Encoders Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multi-class-sentiment-analysis-using-bert-86657a2af156 <br><br>\n",
    "Set up pretrained models for BERT using https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error about a method not being found, try installing these packages. Make sure you have kernel set to myenv or whatever virtual environment you have. virtualenv setup can be found in huggingface installation section of github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install tensorflow\n",
    "#!{sys.executable} -m pip install torch torchvision\n",
    "#!{sys.executable} -m pip install transformers\n",
    "#!{sys.executable} -m pip install tensorflow_datasets\n",
    "#!{sys.executable} -m pip install six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7854, 0.6843, 0.1862],\n",
      "        [0.2826, 0.9062, 0.1041],\n",
      "        [0.8659, 0.7467, 0.9925],\n",
      "        [0.9332, 0.8374, 0.1405],\n",
      "        [0.3772, 0.3838, 0.6026]])\n"
     ]
    }
   ],
   "source": [
    "#verify pytorch installed successfully\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import six\n",
    "print(six.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit for BERT setup code goes to Leo Dirac and his youtube video lecture: [LSTM is dead. Long Live Transformers!](https://www.youtube.com/watch?v=S27pHKBEp30)<br> Make sure pytorch test goes through and tensorflow version is 2.2.0 or later. Follow huggingface's [setup documentation](https://www.tensorflow.org/install/pip#mac-os) using --upgrade in the console if version isn't correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Load dataset info from /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0\n",
      "INFO:absl:Reusing dataset glue (/Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "import tensorflow_datasets\n",
    "\n",
    "#Load Dataset, tokenizer, model from pretrained model/vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "data = tensorflow_datasets.load('glue/mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=.00003, \n",
    "                                     epsilon=1e-08, \n",
    "                                     clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-684b275d0626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n\u001b[0m\u001b[1;32m      2\u001b[0m          validation_data=valid_dataset, validation_steps=7)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n",
    "         validation_data=valid_dataset, validation_steps=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
