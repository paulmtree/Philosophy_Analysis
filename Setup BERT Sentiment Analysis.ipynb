{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Encoders Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multi-class-sentiment-analysis-using-bert-86657a2af156 <br><br>\n",
    "Set up pretrained models for BERT using https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error about a method not being found, try installing these packages. Make sure you have kernel set to myenv or whatever virtual environment you have. virtualenv setup can be found in huggingface installation section of github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/87/c364666684653d45bba918dbdfbf7bff80a84128a312249600bd7d6ef8f6/tensorflow_datasets-3.2.0-py3-none-any.whl (3.4MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4MB 1.5MB/s eta 0:00:01     |██▉                             | 296kB 443kB/s eta 0:00:08\n",
      "\u001b[?25hCollecting dill\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/96/518a8ea959a734b70d2e95fef98bcbfdc7adad1c1e5f5dd9148c835205a5/dill-0.3.2.zip (177kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (3.9.2)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (1.12.0)\n",
      "Requirement already satisfied: absl-py in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (0.8.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (2.21.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (19.1.0)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (1.16.2)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading https://files.pythonhosted.org/packages/91/94/07baf9e2a060fc5c7bd95052cc9114afd3f67b6927bd7224c0626fad5703/tensorflow_metadata-0.22.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (4.46.0)\n",
      "Requirement already satisfied: wrapt in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (1.11.1)\n",
      "Collecting promise\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz\n",
      "Requirement already satisfied: termcolor in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (0.17.1)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow_datasets) (44.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Collecting googleapis-common-protos\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/74/3956721ea1eb4bcf7502a311fdaa60b85bd751de4e57d1943afe9b334141/googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 2.0MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: dill, promise\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.2-cp37-none-any.whl size=78914 sha256=a0e95732d97b6ad839b89dd8847b4fae1e0d84566e2a8283ae86bad89d675372\n",
      "  Stored in directory: /Users/paulmccabe/Library/Caches/pip/wheels/27/4b/a2/34ccdcc2f158742cfe9650675560dea85f78c3f4628f7daad0\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-cp37-none-any.whl size=21494 sha256=43691fb1cc7e0023d995ff52c02d1357b09180f4a6f78a7a057a53839caf91fd\n",
      "  Stored in directory: /Users/paulmccabe/Library/Caches/pip/wheels/19/49/34/c3c1e78bcb954c49e5ec0d31784fe63d14d427f316b12fbde9\n",
      "Successfully built dill promise\n",
      "Installing collected packages: dill, googleapis-common-protos, tensorflow-metadata, promise, tensorflow-datasets\n",
      "Successfully installed dill-0.3.2 googleapis-common-protos-1.52.0 promise-2.3 tensorflow-datasets-3.2.0 tensorflow-metadata-0.22.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install tensorflow\n",
    "#!{sys.executable} -m pip install torch torchvision\n",
    "#!{sys.executable} -m pip install transformers\n",
    "#!{sys.executable} -m pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7788, 0.8197, 0.5021],\n",
      "        [0.2523, 0.8528, 0.7840],\n",
      "        [0.7203, 0.6718, 0.5571],\n",
      "        [0.0915, 0.7567, 0.6972],\n",
      "        [0.6589, 0.6709, 0.0758]])\n"
     ]
    }
   ],
   "source": [
    "#verify pytorch installed successfully\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit for BERT setup code goes to Leo Dirac and his youtube video lecture: [LSTM is dead. Long Live Transformers!](https://www.youtube.com/watch?v=S27pHKBEp30)<br> Make sure pytorch test goes through and tensorflow version is 2.2.0 or later. Follow huggingface's [setup documentation](https://www.tensorflow.org/install/pip#mac-os) using --upgrade in the console if version isn't correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f9acb856ad4c85b9e73c791d2ba2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6084531cd3488c9cd3cd81e385ecb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a27ce5bf2e4e4e82c306445f3bc86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=526681800.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: glue/mrpc/1.0.0\n",
      "INFO:absl:Load dataset info from /var/folders/fp/wvf1hch173l1s3s68ql1zz_r0000gn/T/tmpjno0ha64tfds\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset glue (/Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset glue/mrpc/1.0.0 (download: 1.43 MiB, generated: Unknown size, total: 1.43 MiB) to /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acec76fe304d4678af0163ced9fde702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebf197e71b841c39953b1d5193a73c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc into /Users/paulmccabe/tensorflow_datasets/downloads/fire.goog.com_v0_b_mtl-sent-repr.apps.com_o_2FjSIMlCiqs1QSmIykr4IRPnEHjPuGwAz5i40v8K9U0Z8.tsvalt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc.tmp.39933efdd4db4d27987f62a37839966b...\n",
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt into /Users/paulmccabe/tensorflow_datasets/downloads/dl.fbaip.com_sente_sente_msr_parap_test0PdekMcyqYR-w4Rx_d7OTryq0J3RlYRn4rAMajy9Mak.txt.tmp.76bdcf3478424f8fb46cd961e1129a4b...\n",
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt into /Users/paulmccabe/tensorflow_datasets/downloads/dl.fbaip.com_sente_sente_msr_parap_trainfGxPZuQWGBti4Tbd1YNOwQr-OqxPejJ7gcp0Al6mlSk.txt.tmp.fdd4d2047898482094d190987eb9f3a9...\n",
      "INFO:absl:Generating split train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0.incompleteF0BIGO/glue-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413d90b92b3e47a0a47d33d7e2cd1250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3668.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0.incompleteF0BIGO/glue-train.tfrecord. Shard lengths: [3668]\n",
      "INFO:absl:Generating split validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0.incompleteF0BIGO/glue-validation.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806c67242be04adebc619ef3982498ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0.incompleteF0BIGO/glue-validation.tfrecord. Shard lengths: [408]\n",
      "INFO:absl:Generating split test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0.incompleteF0BIGO/glue-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83711935b5c4843924ccc767af00e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1725.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0.incompleteF0BIGO/glue-test.tfrecord. Shard lengths: [1725]\n",
      "INFO:absl:Skipping computing stats for mode ComputeStatsMode.AUTO.\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset glue downloaded and prepared to /Users/paulmccabe/tensorflow_datasets/glue/mrpc/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "import tensorflow_datasets\n",
    "\n",
    "#Load Dataset, tokenizer, model from pretrained model/vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "data = tensorflow_datasets.load('glue/mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=.00003, \n",
    "                                     epsilon=1e-08, \n",
    "                                     clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-684b275d0626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n\u001b[0m\u001b[1;32m      2\u001b[0m          validation_data=valid_dataset, validation_steps=7)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n",
    "         validation_data=valid_dataset, validation_steps=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
